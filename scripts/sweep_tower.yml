# wandb variables
program: sweep.py
project: neural_fofin_tower
method: random  # grid, random
metric:
  goal: minimize
  name: loss
# parameters to be sweeped
parameters:
  model:
    value: "formfinder"  # Supported models are formfinder, autoencoder, and piggy
  # randomness
  seed:
    value: 91
  # data generator
  generator:
    # we need to specify "parameters" again in every nested field of hyperparameters
    # a wandb sweep would not work otherwise
    parameters:
      name:
        value: "tower_circle"  # options: tower_ellipse, tower_circle
      bounds:
        value: "straight"  # options: straight, twisted
      height:
        value: 10.0
      radius:
        value: 2.5
      num_sides:
        value: 16
      num_levels:
        value: 21
      num_rings:
        value: 3
  # fd simulation
  fdm:
    parameters:
      load:
        value: 0.0
  # encoder
  encoder:
    parameters:
      hidden_layer_size:
        values: [128, 256, 512, 1024]
      hidden_layer_num:
        values: [3, 4, 5]
      activation_fn_name:
        value: "elu"
      final_activation_fn_name:
        value: "softplus"
  # decoder
  decoder:
    parameters:
      include_params_xl:
        value: True
      hidden_layer_size:
        values: [128, 256, 512, 1024]
      hidden_layer_num:
        values: [3, 4, 5]
      activation_fn_name:
        value: "elu"
  # loss
  loss:
    parameters:
      shape:
        parameters:
          include:
            value: True
          weight:
            value: 1.0
          scale:
            value: 1.0
      energy:
        parameters:
          include:
            value: True
          weight:
            value: 1.0
          scale:
            value: 1.0
      residual:
        parameters:
          include:
            value: True
          weight:
            value: 1.0
          scale:
            value: 1.0
  # optimizer
  optimizer:
    parameters:
      name:
        value: "adam"
      learning_rate:
        # NOTE: Be careful with scientific notation in YAML!
        values: [1.0e-4, 3.0e-4, 5.0e-4, 1.0e-5, 3.0e-5, 5.0e-5]
  # training
  training:
    parameters:
      steps:
        value: 5000
      batch_size:
        value: 64
