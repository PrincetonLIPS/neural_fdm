# Real-time design of architectural simulations with differentiable mechanics and neural networks

_Code base for the [paper](https://arxiv.org/abs/2409.02606) published at ICLR 2025_

![Our trained model, deployed in Rhino3D](masonry_vault_cad_design.gif)

## 0. Overview

This repository contains two folders with the meat of our work: `src` and `scripts`.

The first folder, `src`, defines all the code infrastructure we need to build, train, serialize, and visualize our model and the baselines.
The second one, `scripts`, groups a list of routines to execute the code in `src`, and more importantly, to reproduce our experiments at inference time.

With the scripts, you can even tesselate and 3D print your own masonry vault from one of our model predictions if you fancy!

## 1. Installation

>We only support installation on a CPU. Our paper does not use any GPUs. Crazy, right?

Create a new [Anaconda](https://www.anaconda.com/) environment and then activate it:

```bash
conda create -n neural
conda activate neural
```

Install some dependencies from `pip`-land:

```bash
pip install --upgrade jax==0.4.23
pip install optax==0.1.5 equinox==0.11.3
pip install seaborn
```

Next, install COMPAS and COMPAS VIEW2 via `conda`. Please mind the version of these dependencies:

```bash
conda install -c conda-forge compas<2.0 compas_view2==0.7.0 
```

Finally, clone and install this repository from source:

```bash
git clone https://github.com/arpastrana/neural_fdm.git
cd neural_fdm
pip install -e .
```
Installing this repo from source will take care of installing the few additional dependencies listed in `requirements.txt`.
Now, go ahead and play. Rock and roll üé∏! 


## 2. Configuration files

Our work focuses on two structural design tasks: compression-only shells and cablenet towers.
We thus create a `.yml` file with all the configuration hyperparameters per task.

The files are stored in the `scripts` folder as:
- `bezier.yml`, and
- `tower.yml` 

for the first and the second task, respectively.
The hyperparameters exposed in the configuration files range from choosing a data generator, prescribing the model architecture, and the optimization scheme.
We'll be mingling with them to steer the wheel while we run experiments.


### 2.1 Data generation

An advantage of our work is that we only need to define target shapes alone, without a vector of force densities to be paired as ground-truth labels.
That would be the case in a fully supervised setting, which is not the case here.
Our model figures these labels out automatically.
This allows us to generate a dataset of target shapes on the fly at train time by specifying a `generator` configuration and a random `seed` to create pseudo-random keys.

#### Shells
The target shell shapes are parametrized by a square Bezier patch.

- `name`: The name of the generator to instantiate. One of `bezier_symmetric_double`, `bezier_symmetric`, `bezier_asymmetric`, and `bezier_lerp`. The first two options constraint the shapes to be symmetric along two or one axis, respectively. The third option does not enforce symmetry. The last option, `bezier_lerp` is used to interpolate linearnly a batch of doubly-symmetric and asymmetric shapes (i.e., shapes generated by `bezier_symmetric_double` and `bezier_asymmetric`).
- `num_points`: the number of control points that parametrize the Bezier patch.
- `bounds`: It specifies how to wiggle the control points of the patch on a `num_points x num_points` grid. The option `pillow` only moves the internal control point up and down, while `dome` additionally jitters the two control points on the boundary in and out. `saddle` is an extension of `dome` in that it lets one of the control points on the boundary move up and down too.
- `num_uv`: The number of spans to evaluate on the Bezier along the *u* and *v* directions. A value of `10`, for example, would result in a `10x10` grid of target points. These are the points to be matched during training.
- `size`: The length of the sides of the patch. It defines the scale of the task.
- `lerp_factor`: A scalar factor in [0, 1] to interpolate between two target surfaces. Only employed for `bezier_lerp`.

#### Towers
The target tower shapes are described in turn by a vertical sequence of planar circles.
The tower rings are deformed and rotated depending on the generator `name` and `bounds`.

- `name`: The generator name. Use `tower_ellipse` to make target shapes with elliptical rings, and `tower_circles` to keep the rings as circles.
- `bounds`: Either `straight` or `twisted`. The former scales the rings on the plane at random. The latter scales and rotates the rings randomly.
- `height`: The tower height.
- `radius`: The start radius of the all the generated circles.
- `num_sides`: The number of segments to discretize each circle with.
- `num_levels`: The number of circles to create along the tower's height. Equidistantly spaced.
- `num_rings`: The number of circles to be morphed during training. Must be `>2` since two of these rings are, by default, the top and bottom of the tower.

### 2.2 Building a model

We specify the architecture of a model in the configuration file, which for the most part, ressembles an autoencoder.
The configuration scheme is the same for any task.

#### Neural networks

Our experiments use multilayer perceptrons (MLP) for the encoder that maps shapes to simulation parameters, although we are by no means restricted to that.
An MLP too serves as a decoder for our fully neural baselines.
We employ one of the simplest possible neural networks, the MLP, to quantify the benefits of having a physics simulator in a neural network in large-scale mechanical design tasks.
This sets a baseline from which we can build upon with beefier architectures like graph neural networks, transformers, and beyond.

The encoder hyperparameters are:
- `shift`: The lower bound shift in output of the last layer of the encoder. This is precisely what we call `tau` in the paper.
- `hidden_layer_size`: The width of every fully-connected hidden layer. We restrict the size to `256` in all the experiments.
- `hidden_layer_num`: The number of hidden layers, output layer inclusive.
- `activation_fn_name`: The name of the activation function after each hidden layer. We typically resort to `elu`.
- `final_activation_fn_name`: The activation function name after the output layer. We use `softplus` to ensure a strictly positive output, as needed by the simulator decoder.

The neural decoder's setup mirrors the encoder's, except for the `include_params_xl` flag. If set to `True`, then the decoder expects the latents and boundary conditions as inputs. Otherwise, it only decodes the latents. We fix this hyperparameter to `True` in the paper.

#### Simulator

For the simulator, the force density method (FDM), we only have `load` as a hyperparameter, which sets the magnitude of a vertical **area** load applied to the structures in the direction of gravity (hello Isaac Newton! üçé).

If this value is nonzero, then the model will convert the area load into point loads to be compatible with our physics simulator.

### 2.3 Living and learning

The training setup is also defined in the configuration file of the task, including the `loss` function to optimize for, the `optimizer` that updates the model parameters, and the `training` schedule that pretty much allocates the compute budget.

The `loss` function is the sum of multiple terms, that for the most part are a shape loss and a physics loss, as we explain in the paper.
We allow for more refined control on the scaling of each loss term in the file:
- `include`: Whether or not to include the loss term during training. If set to `False`, then the value of the loss term is not calculated, saving some computation resources. By default, `include=True`.
- `weight`: The scalar weight of the loss term used for callibrating model performance, called `kappa` in the paper. It is particularly useful to tune the scale of the physics loss whent raining the PINN baseline. The weight is `weight=1.0` by default unless otherwise stated.
- `scale`: An additional scalar used for normalization of the loss function values w.r.t. the bounding box of the design space, or the magnitude of the applied loads. We end up not using this hyperparameter (i.e., we set to to `1.0`).

The `optimizer` hyperparameters are:
- `name`: the name of the gradient-based optimizer. We currently support `adam` and `sgd` from the `optax` library, but only use `adam` in the paper.
- `learning_rate`: The constant learning rate. The rate is fixed, we ommit schedulers - it is more elegant.
- `clip_norm`: The global norm for gradient clipping. If set to `0.0`, then gradient clipping is ignored.

And for the `training` routine:
- `steps`: The number of optimization steps to train a model for (i.e., the number of times the model parameters are updated). We mostly train the models for `10000` steps.
- `batch_size`: The batch size of the input data.


## Training

After setting up the config files, now it's time to make that CPU go brrrrr.
Execute the `train.py` script from your terminal:

```bash
python train.py <model_name> <task_name>
```

Where `task_name` is either `bezier` for the shells task or `tower` for the towers task.

The `model_name` is where things get interesting. 
In summary:

- Ours: `formfinder`
- NN baseline: `autoencoder`
- PINN baseline: `autoencoder_pinn` 

Our model is called `formfinder`, and the fully neural baseline is called `autoencoder`.
Input the name of the model you wish to train. If `autoencoder` is trained with the `residual` (i.e., the physics loss is included and active), this model will become a PINN baseline and will internally be renamed as `autoencoder_pinn` (sorry, naming is hard).

Task-specific configuration details are given in the paper.

We invite you to check the docstring of the `train.py` script to see all the input options. 
They would allow you to warmstart the training from an existing pretrained model, checkpoint every so often, as well as plot and export the loss history for your inspection.

> A note on hyperparameter tuning. We utilized WandB to run hyperparameter sweeps. The sweeps are in turn handled by the `sweep.py` script in tandem with `sweep_bezier.yml` or `sweep_tower.yml` files, depending on the task. The structure of these sweep files mimics that of the configuration files described herein. We trust you'll be able to find your way around them if you really want to fiddle with them. 

## Testing

Blob.

## Visualization

View.

### Direct optimization

Another baseline.


Blah.

## Citation

Consider citing our paper if this work was helpful to your research.
Don't worry, it's free.

```bibtex
@inproceedings{
    pastrana_2025_diffmechanics,
    title={Real-time design of architectural structures with differentiable mechanics and neural networks},
    author={Rafael Pastrana and Eder Medina and Isabel M. de Oliveira and Sigrid Adriaenssens and Ryan P Adams},
    booktitle={The Thirteenth International Conference on Learning Representations},
    year={2025},
    url={https://openreview.net/forum?id=Tpjq66xwTq}
}
```

## Contact

Reach out! If you have questions or find bugs in our code, please open an issue on Github or email the authors at arpastrana@princeton.edu. 